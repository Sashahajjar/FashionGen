{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS0MXBm1stRn"
      },
      "source": [
        "# Fashion-Gen Multi-Modal Training on Google Colab\n",
        "\n",
        "This notebook trains the CNN + RNN + Fusion model on Fashion-Gen data from Kaggle.\n",
        "\n",
        "**Features:**\n",
        "- Clones project from GitHub automatically\n",
        "- Option to test with mock data (no download needed)\n",
        "- Downloads only validation H5 file (saves disk space)\n",
        "- Streaming HDF5 dataset (no RAM crash)\n",
        "- Subset training with `--max_samples` option\n",
        "- Robust error handling and debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-G2a8TIstVo"
      },
      "source": [
        "## 1. Clone Project from GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nok_5eSKstWw",
        "outputId": "90ae5614-27d3-421e-a334-83acad859312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Clone the project from GitHub\n",
        "!git clone https://github.com/Sashahajjar/FashionGen.git\n",
        "\n",
        "# Set project directory\n",
        "import os\n",
        "import sys\n",
        "PROJECT_DIR = '/content/FashionGen'\n",
        "os.chdir(PROJECT_DIR)\n",
        "sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "print(f\"✓ Project cloned to: {PROJECT_DIR}\")\n",
        "print(f\"✓ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmcSpu3ustW0",
        "outputId": "74cb9bf8-2816-4b00-e1cd-cabc2df8b3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "# IMPORTANT: Run this cell, then RESTART RUNTIME (Runtime → Restart runtime)\n",
        "# After restart, skip this cell and go to the next one\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Step 1: Uninstalling incompatible packages...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\", \"torch\", \"torchvision\", \"scipy\", \"scikit-learn\"])\n",
        "\n",
        "print(\"\\nStep 2: Installing compatible versions...\")\n",
        "# Install numpy first, then torch/torchvision\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.4\", \"--no-cache-dir\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"--no-cache-dir\"])\n",
        "\n",
        "print(\"\\nStep 3: Installing other dependencies...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\", \"Pillow\", \"scikit-learn\", \"h5py\", \"kaggle\", \"--no-cache-dir\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"⚠️  IMPORTANT: RESTART RUNTIME NOW!\")\n",
        "print(\"Runtime → Restart runtime\")\n",
        "print(\"Then SKIP this cell and continue to next cell\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RztjEWBstW4"
      },
      "source": [
        "## 2. Environment Setup & GPU Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "1-gOzj-KstW5",
        "outputId": "1e2f349b-637d-4e77-f52f-c5b90f5a625f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your kaggle.json file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e653c105-3f3d-4ca9-8232-5b3d85d04211\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e653c105-3f3d-4ca9-8232-5b3d85d04211\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "✓ Found kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Detect GPU and print device info\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)  # Suppress numpy compatibility warnings\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU (training will be slower)\")\n",
        "\n",
        "# Test imports\n",
        "print(\"\\nTesting imports...\")\n",
        "from data.dataset import FashionGenDataset\n",
        "from models.fusion_model import create_fusion_model\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQYBgbXDstW7",
        "outputId": "c38bebc2-44db-4bd2-f935-4412a91089ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Kaggle credentials configured\n",
            "✓ File permissions: 600\n"
          ]
        }
      ],
      "source": [
        "## 3. Choose: Mock Data or Real Data\n",
        "\n",
        "**Option A: Test with Mock Data (Quick, No Download)**\n",
        "- Skip to Section 4\n",
        "- No Kaggle account needed\n",
        "- Fast testing\n",
        "\n",
        "**Option B: Use Real FashionGen Data**\n",
        "- Continue with Kaggle authentication below\n",
        "- Requires Kaggle account\n",
        "- Downloads ~1.7GB validation file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrq1gGSAstXA",
        "outputId": "04919d89-a811-44c4-c484-1c7bf3673b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                               title                                           size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------  ----------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "neurocipher/heartdisease                          Heart Disease                                   3491  2025-12-11 15:29:14.327000           2114        179  1.0              \n",
            "kundanbedmutha/exam-score-prediction-dataset      Exam Score Prediction Dataset                 325454  2025-11-28 07:29:01.047000           5863        222  1.0              \n",
            "dansbecker/melbourne-housing-snapshot             Melbourne Housing Snapshot                    461423  2018-06-05 12:52:24.087000         200095       1720  0.7058824        \n",
            "\n",
            "✓ Kaggle authentication verified\n"
          ]
        }
      ],
      "source": [
        "## 3A. Kaggle Authentication (Skip if using mock data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEcv8wYhstXC"
      },
      "source": [
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Find the uploaded kaggle.json\n",
        "kaggle_json = None\n",
        "for filename in uploaded.keys():\n",
        "    if 'kaggle' in filename.lower() and filename.endswith('.json'):\n",
        "        kaggle_json = filename\n",
        "        break\n",
        "\n",
        "if not kaggle_json:\n",
        "    raise FileNotFoundError(\"kaggle.json file not found in uploads\")\n",
        "\n",
        "print(f\"✓ Found {kaggle_json}\")\n",
        "\n",
        "# Move kaggle.json to ~/.kaggle/ and set permissions\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "shutil.copy(kaggle_json, '/root/.kaggle/kaggle.json')\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "print(\"✓ Kaggle credentials configured\")\n",
        "print(f\"✓ File permissions: {oct(os.stat('/root/.kaggle/kaggle.json').st_mode)[-3:]}\")\n",
        "\n",
        "# Verify Kaggle authentication\n",
        "!kaggle datasets list | head -5\n",
        "print(\"\\n✓ Kaggle authentication verified\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml-OnsMcstXD",
        "outputId": "8f22e02e-5ef0-4808-d179-0c102537e4b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "overlay         113G   39G   75G  35% /\n",
            "\n",
            "✓ Data directory: /content/kaggle_data\n",
            "✓ Free disk space: 74.13 GB\n",
            "✓ Sufficient disk space available\n"
          ]
        }
      ],
      "source": [
        "# Check disk space before download\n",
        "!df -h / | tail -1\n",
        "\n",
        "# Create data directory on Colab local disk (NOT Google Drive)\n",
        "DATA_DIR = '/content/kaggle_data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n✓ Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Get free space in GB\n",
        "import shutil\n",
        "total, used, free = shutil.disk_usage('/')\n",
        "free_gb = free / (1024**3)\n",
        "print(f\"✓ Free disk space: {free_gb:.2f} GB\")\n",
        "\n",
        "# Warn if less than 5GB free (validation file is ~2-3GB)\n",
        "if free_gb < 5:\n",
        "    print(\"⚠ WARNING: Low disk space! May cause download failures.\")\n",
        "else:\n",
        "    print(\"✓ Sufficient disk space available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3bX0iaPstXE",
        "outputId": "f848c62e-d5d4-4009-c7b1-16ac08626d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available files in Fashion-Gen dataset:\n",
            "name                                     size  creationDate                \n",
            "--------------------------------  -----------  --------------------------  \n",
            "fashiongen_256_256_train.h5       14387677469  2023-05-27 05:27:54.521000  \n",
            "fashiongen_256_256_validation.h5   1793119395  2023-05-27 05:19:00.258000  \n"
          ]
        }
      ],
      "source": [
        "# List available files in Fashion-Gen dataset\n",
        "print(\"Available files in Fashion-Gen dataset:\")\n",
        "!kaggle datasets files bothin/fashiongen-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXlIXaSSstXI",
        "outputId": "baaa56fd-a03c-41fd-e89e-0dd1495aac61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading fashiongen_256_256_validation.h5...\n",
            "This may take a few minutes...\n",
            "Note: Kaggle may return a zip file - we'll handle that automatically\n",
            "\n",
            "✓ Download command completed\n",
            "Dataset URL: https://www.kaggle.com/datasets/bothin/fashiongen-validation\n",
            "License(s): unknown\n",
            "Downloading fashiongen_256_256_validation.h5 to /content/kaggle_data\n",
            "\n",
            "\n",
            "\n",
            "✓ File downloaded: 1.47 GB\n"
          ]
        }
      ],
      "source": [
        "# Download ONLY validation H5 file (smaller, faster)\n",
        "# Prefer validation file: fashiongen_256_256_validation.h5\n",
        "H5_FILENAME = 'fashiongen_256_256_validation.h5'\n",
        "EXPECTED_SIZE_GB = 1.5  # Expected size in GB\n",
        "\n",
        "# Clean up any existing corrupted files\n",
        "existing_file = os.path.join(DATA_DIR, H5_FILENAME)\n",
        "if os.path.exists(existing_file):\n",
        "    print(f\"⚠ Found existing file, checking if valid...\")\n",
        "    file_size_gb = os.path.getsize(existing_file) / (1024**3)\n",
        "    if file_size_gb < 0.5:  # Too small, likely incomplete\n",
        "        print(f\"⚠ File too small ({file_size_gb:.2f} GB), deleting...\")\n",
        "        os.remove(existing_file)\n",
        "    else:\n",
        "        print(f\"Existing file size: {file_size_gb:.2f} GB (expected ~{EXPECTED_SIZE_GB} GB)\")\n",
        "\n",
        "print(f\"\\nDownloading {H5_FILENAME}...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "print(\"Note: Kaggle may return a zip file - we'll handle that automatically\")\n",
        "\n",
        "try:\n",
        "    import subprocess\n",
        "    result = subprocess.run(\n",
        "        ['kaggle', 'datasets', 'download', '-d', 'bothin/fashiongen-validation',\n",
        "         '-f', H5_FILENAME, '-p', DATA_DIR],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=1800  # 30 minute timeout\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n✓ Download command completed\")\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "    else:\n",
        "        print(f\"\\n⚠ Download command returned code {result.returncode}\")\n",
        "        if result.stderr:\n",
        "            print(\"Errors:\", result.stderr)\n",
        "        if result.stdout:\n",
        "            print(\"Output:\", result.stdout)\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"\\n✗ Download timed out (>30 minutes)\")\n",
        "    print(\"This may indicate network issues or very large file\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Download failed: {e}\")\n",
        "    print(\"\\nDebugging info:\")\n",
        "    !df -h /\n",
        "    !ls -lh {DATA_DIR}\n",
        "    !ls -la ~/.kaggle\n",
        "    raise\n",
        "\n",
        "# Verify download completed\n",
        "downloaded_file = os.path.join(DATA_DIR, H5_FILENAME)\n",
        "if os.path.exists(downloaded_file):\n",
        "    file_size_gb = os.path.getsize(downloaded_file) / (1024**3)\n",
        "    print(f\"\\n✓ File downloaded: {file_size_gb:.2f} GB\")\n",
        "    if file_size_gb < 0.5:\n",
        "        print(\"⚠ WARNING: File seems too small, download may be incomplete!\")\n",
        "else:\n",
        "    print(\"⚠ File not found with expected name, checking for zip files...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XyXxbHuyr6F",
        "outputId": "c75c82ef-7769-4aca-863f-40f83adf3365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.5G\n",
            "-rw-r--r-- 1 root root 1.5G May 27  2023 fashiongen_256_256_validation.h5\n"
          ]
        }
      ],
      "source": [
        "ls -lh /content/kaggle_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CCn4wqJzy0sz"
      },
      "outputs": [],
      "source": [
        "!mv /content/kaggle_data/fashiongen_256_256_validation.h5 /content/kaggle_data/fashiongen_256_256_validation.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1RWm3GZzaVP",
        "outputId": "385e177f-3c79-45b6-ec4f-7932e5fabc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/kaggle_data/fashiongen_256_256_validation.zip\n",
            "  inflating: /content/kaggle_data/fashiongen_256_256_validation.h5  \n"
          ]
        }
      ],
      "source": [
        "!unzip -n /content/kaggle_data/fashiongen_256_256_validation.zip -d /content/kaggle_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EILAvCFJzjoF",
        "outputId": "0ccc0313-ff8f-414e-8f2e-4bfc6df1274d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.7G\n",
            "-rw-r--r-- 1 root root 1.7G May 27  2023 fashiongen_256_256_validation.h5\n"
          ]
        }
      ],
      "source": [
        "!rm -f /content/kaggle_data/fashiongen_256_256_validation.zip\n",
        "!ls -lh /content/kaggle_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTO1iokJzmVz",
        "outputId": "ec59f95c-6af1-4859-c512-6bb19d4a5e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Opened H5\n",
            "Keys: ['index', 'index_2', 'input_brand', 'input_category', 'input_composition', 'input_concat_description', 'input_department', 'input_description', 'input_gender', 'input_image', 'input_msrpUSD', 'input_name', 'input_pose', 'input_productID', 'input_season', 'input_subcategory']\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "path = \"/content/kaggle_data/fashiongen_256_256_validation.h5\"\n",
        "with h5py.File(path, \"r\") as f:\n",
        "    print(\"✅ Opened H5\")\n",
        "    print(\"Keys:\", list(f.keys())[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge8Xhr9Y025b",
        "outputId": "13ddf677-8ca3-413d-ba5c-eb2a376e6ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ H5 file path set: /content/kaggle_data/fashiongen_256_256_validation.h5\n"
          ]
        }
      ],
      "source": [
        "# Set H5 file path (you already verified it works)\n",
        "H5_FILE_PATH = \"/content/kaggle_data/fashiongen_256_256_validation.h5\"\n",
        "print(f\"✅ H5 file path set: {H5_FILE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Y21F0i1VuH",
        "outputId": "67b03bb6-713f-4eec-b161-b57adb3ec1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image: (32528, 256, 256, 3) uint8\n",
            "text : (32528, 1) |S400\n",
            "label: (32528, 1) |S100\n",
            "sample label: [b'JACKETS & COATS']\n",
            "sample text : [b'Denim-like jogg jacket in blue. Fading and whiskering throughout. Spread collar. Copper tone button closures at front. Flap pockets at chest with metallic logo plaque. Seam pockets at sides. Cinch tabs at back waistband. Single button sleeve cuffs. Tone on tone stitching.']\n",
            "image min/max: 9 255\n"
          ]
        }
      ],
      "source": [
        "# Quick test: Verify dataset loader works\n",
        "import sys\n",
        "sys.path.insert(0, '/content/fashiongen-project')\n",
        "\n",
        "from data.h5_dataset import FashionGenH5Dataset\n",
        "\n",
        "test_dataset = FashionGenH5Dataset(\n",
        "    h5_file_path=H5_FILE_PATH,\n",
        "    image_size=(224, 224),\n",
        "    max_seq_len=50,\n",
        "    vocab_size=10000,\n",
        "    split='train',\n",
        "    max_samples=10\n",
        ")\n",
        "\n",
        "sample = test_dataset[0]\n",
        "print(f\"✅ Dataset works! Image: {sample['image'].shape}, Label: {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up disk space - delete cache and temporary files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"Cleaning up disk space...\\n\")\n",
        "\n",
        "# 1. Clear pip cache\n",
        "print(\"1. Clearing pip cache...\")\n",
        "!pip cache purge 2>/dev/null || echo \"No pip cache found\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# 2. Clear Python cache (__pycache__)\n",
        "print(\"2. Removing Python cache files...\")\n",
        "!find /content -type d -name __pycache__ -exec rm -r {} + 2>/dev/null || echo \"No cache found\"\n",
        "!find /content -name \"*.pyc\" -delete 2>/dev/null || echo \"No .pyc files found\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# 3. Clear temporary files\n",
        "print(\"3. Clearing temporary files...\")\n",
        "!rm -rf /tmp/* 2>/dev/null || echo \"No temp files\"\n",
        "!rm -rf /content/tmp/* 2>/dev/null || echo \"No content/tmp files\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# 4. Clear Colab cache\n",
        "print(\"4. Clearing Colab cache...\")\n",
        "!rm -rf /root/.cache/* 2>/dev/null || echo \"No cache\"\n",
        "!rm -rf /root/.local/share/Trash/* 2>/dev/null || echo \"No trash\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# 5. Clear matplotlib cache\n",
        "print(\"5. Clearing matplotlib cache...\")\n",
        "!rm -rf /root/.cache/matplotlib/* 2>/dev/null || echo \"No matplotlib cache\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# 6. Clear torch cache (if not needed)\n",
        "print(\"6. Clearing torch cache...\")\n",
        "!rm -rf /root/.cache/torch/* 2>/dev/null || echo \"No torch cache\"\n",
        "print(\"✅ Done\\n\")\n",
        "\n",
        "# Show disk space after cleanup\n",
        "print(\"\\n=== Disk Space After Cleanup ===\")\n",
        "!df -h / | tail -1\n",
        "\n",
        "total, used, free = shutil.disk_usage('/')\n",
        "free_gb = free / (1024**3)\n",
        "print(f\"\\n✅ Free space: {free_gb:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_MC0FubstXK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFugPphgstXK"
      },
      "outputs": [],
      "source": [
        "# Project already cloned in Cell 2\n",
        "# Just verify it's set up correctly\n",
        "print(f\"✓ Project directory: {PROJECT_DIR}\")\n",
        "print(f\"✓ Current directory: {os.getcwd()}\")\n",
        "!ls -la {PROJECT_DIR} | head -10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2YG24txstXM"
      },
      "outputs": [],
      "source": [
        "# Setup Python path and test imports\n",
        "import sys\n",
        "sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "from data.h5_dataset import FashionGenH5Dataset\n",
        "from models.fusion_model import create_fusion_model\n",
        "print(\"✅ Imports work\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0sRHWMstXM"
      },
      "source": [
        "## 4. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Vnf5OHstXN"
      },
      "outputs": [],
      "source": [
        "# Training config\n",
        "USE_MOCK_DATA = True  # Set to False to use real HDF5 data\n",
        "MAX_SAMPLES = 100  # Use subset for quick testing (None = all samples)\n",
        "BATCH_SIZE = 32 if torch.cuda.is_available() else 16\n",
        "\n",
        "print(f\"Use mock data: {USE_MOCK_DATA}\")\n",
        "print(f\"Max samples: {MAX_SAMPLES}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Set H5 file path (only used if USE_MOCK_DATA = False)\n",
        "if not USE_MOCK_DATA:\n",
        "    H5_FILE_PATH = \"/content/kaggle_data/fashiongen_256_256_validation.h5\"\n",
        "    print(f\"H5 file path: {H5_FILE_PATH}\")\n",
        "else:\n",
        "    H5_FILE_PATH = None\n",
        "    print(\"Using mock data - no H5 file needed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmQSmgY3stXO"
      },
      "source": [
        "## 5. Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4T8bc94stXO"
      },
      "outputs": [],
      "source": [
        "# Run training\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "if USE_MOCK_DATA:\n",
        "    # Test with mock data (no H5 file needed)\n",
        "    cmd = f\"python training/train.py --max_samples {MAX_SAMPLES}\"\n",
        "    print(f\"Running with MOCK DATA: {cmd}\\n\")\n",
        "else:\n",
        "    # Use real HDF5 data\n",
        "    cmd = f\"python training/train.py --h5_file {H5_FILE_PATH} --max_samples {MAX_SAMPLES}\"\n",
        "    print(f\"Running with REAL DATA: {cmd}\\n\")\n",
        "\n",
        "!{cmd}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syhqBfygstXP"
      },
      "source": [
        "## 6. Check Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flj4uHCmstXQ"
      },
      "outputs": [],
      "source": [
        "# Check results\n",
        "!ls -lh {PROJECT_DIR}/saved_models/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jfPoghGstXQ"
      },
      "source": [
        "## 8. Debug Section (if errors occur)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3D1PejIstXQ"
      },
      "outputs": [],
      "source": [
        "# Run this cell if you encounter errors\n",
        "\n",
        "print(\"=== Disk Space ===\")\n",
        "!df -h /\n",
        "\n",
        "print(\"\\n=== Data Directory ===\")\n",
        "!ls -lh {DATA_DIR}\n",
        "\n",
        "print(\"\\n=== Kaggle Auth ===\")\n",
        "!ls -la ~/.kaggle\n",
        "\n",
        "print(\"\\n=== Project Structure ===\")\n",
        "!find {PROJECT_DIR} -type f -name \"*.py\" | head -10\n",
        "\n",
        "print(\"\\n=== Common Issues ===\")\n",
        "print(\"1. 403 Forbidden: Check kaggle.json credentials\")\n",
        "print(\"2. Disk full: Delete old files or use smaller dataset\")\n",
        "print(\"3. Download stuck: Check network, try again\")\n",
        "print(\"4. Import errors: Verify all project files uploaded\")\n",
        "print(\"5. H5 file corrupted: Re-download the file\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
